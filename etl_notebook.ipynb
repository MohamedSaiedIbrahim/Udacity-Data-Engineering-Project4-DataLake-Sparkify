{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed2d0697",
   "metadata": {},
   "source": [
    "## Required Imports & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1a92933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46c915",
   "metadata": {},
   "source": [
    "## Read Configuration file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b791a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read_file(open('dl.cfg'))\n",
    "\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73155ec1",
   "metadata": {},
   "source": [
    "## Define Spark Session and Context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e67de",
   "metadata": {},
   "source": [
    "spark = SparkSession \\\n",
    "        .builder  \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.4\") \\\n",
    "        .getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913b1ae3",
   "metadata": {},
   "source": [
    "## Read the Song data: s3://udacity-dend/song_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd12408b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "song_data = \"s3a://udacity-dend/song_data/*/*/*/*.json\"\n",
    "\n",
    "# read song data file\n",
    "df = spark.read.json(song_data)\n",
    "df.printSchema()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf0466",
   "metadata": {},
   "source": [
    "## Create the 1st Dimension table songs and write into Parquet file insode my S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e96b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create songs table (1st dimension table)\n",
    "#song_id, title, artist_id, year, duration\n",
    "songs_table = df.select(['song_id','title','artist_id','year','duration'])\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.partitionBy(['year','artist_id']).parquet(\"s3a://udacity-datalake-msaied/Project4_Sparkify_DataLake/songs.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fa4044",
   "metadata": {},
   "source": [
    "## Create the 2nd Dimension table Artists and write into Parquet file insode my S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d78141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create artists table (2nd dimension table)\n",
    "#artist_id, name, location, lattitude, longitude\n",
    "artists_table = df.selectExpr(\"artist_id\", \"artist_name as name\", \"artist_location as location\", \"artist_latitude as latitude, artist_longitude as logitude\")\n",
    "\n",
    "# write artists table to parquet files\n",
    "artists_table.write.parquet(\"s3a://udacity-datalake-msaied/Project4_Sparkify_DataLake/artist.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baacb91",
   "metadata": {},
   "source": [
    "## Read the Log data: s3://udacity-dend/log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0718764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "log_data = \"s3a://udacity-dend/log-data/*.json\"\n",
    "\n",
    "# read log data file\n",
    "df = spark.read.json(log_data)\n",
    "\n",
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == 'NextSong')\n",
    "df.printSchema()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15d1d77",
   "metadata": {},
   "source": [
    "## Create the 3rd Dimension table Users and write into Parquet file insode my S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4449ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns for users table (3rd dimension table)\n",
    "#user_id, first_name, last_name, gender, level\n",
    "users_table = df.selectExpr('userId as user_id','firstName as first_name','lastName as last_name','gender','level').distinct()\n",
    "\n",
    "# write users table to parquet files\n",
    "users_table.write.parquet(\"s3a://udacity-datalake-msaied/Project4_Sparkify_DataLake/users.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59f804a",
   "metadata": {},
   "source": [
    "## Create the 4th Dimension table Time and write into Parquet file insode my S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ec6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column\n",
    "get_timestamp = udf(lambda x: datetime.fromtimestamp(x / 1000).replace(microsecond=0), TimestampType())\n",
    "df = df.withColumn(\"start_time\", get_timestamp(\"ts\"))\n",
    "\n",
    "# extract columns to create time table (4th dimension table)\n",
    "#start_time, hour, day, week, month, year, weekday\n",
    "time_table = df.select(['start_time'])\n",
    "time_table = time_table.withColumn(\"hour\", hour(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"day\", dayofmonth(\"start_time\"))\n",
    "time_table = time_table.withcolumn(\"week\", weekofyear(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"month\", month(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"year\", year(\"start_time\"))\n",
    "time_table = time_table.withColumn(\"weekday\", dayofweek(\"start_time\"))\n",
    "\n",
    "time_table = time_table.select(\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\").distinct()\n",
    "\n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.partitionBy(['year','month']).parquet(\"s3a://udacity-datalake-msaied/Project4_Sparkify_DataLake/time.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc906524",
   "metadata": {},
   "source": [
    "## Create the Fact table SongPlays and write into Parquet file insode my S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df = spark.read.parquet(\"s3a://udacity-datalake-msaied/Project4_Sparkify_DataLake/songs.parquet\")\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "#songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent\n",
    "songplays_df = df.join( \\\n",
    "    song_df, \\\n",
    "    [ \\\n",
    "        df.song == song_df.title, \\\n",
    "        df.artist = song_df.name, \\\n",
    "        df.length == song_df.duration \\\n",
    "    ],\"left\" \\\n",
    ")\n",
    "\n",
    "songplays_table = songplays_df.join(time_table, \"start_time\", \"left\") \\\n",
    "    .select( \\\n",
    "        \"start_time\", \\\n",
    "        col(\"userId\").alias(\"user_id\"), \\\n",
    "        \"level\", \\\n",
    "        \"song_id\", \\\n",
    "        \"artist_id\", \\\n",
    "        col(\"sessionId\").alias(\"session_id\"), \\\n",
    "        \"location\", \\\n",
    "        col(\"userAgent\").alias(\"user_agent\"), \\\n",
    "        \"year\", \\\n",
    "        \"month\" \\\n",
    "    )\n",
    "\n",
    "songplays_table.withColumn(\"songplay_id\",monotonically_increasing_id())\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.partitionBy(['year','month']).parquet(\"s3a://udacity-datalake-msaied/Project4_Sparkify_DataLake/songplays.parquet\", mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
